{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('.venv')"
  },
  "interpreter": {
   "hash": "baa918739c774d9c3bb3aa1b7f679fb03ab235703808bce5539ac4b7a3d43177"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    ">Deep Learning is about learning representations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Perceptron: weights are motorized potentiometers. It was a two-layer neural net.\n",
    "\n",
    "$$ y = \\text{sign}\\left(\\sum_{i=1}^N w_i x_i + b\\right) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Traditional pattern recognition uses hand-engineered feature extraction; these features are fed into a trainable model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Deep Learning: The entire task is learned end-to-end. There is an internal representation of the inputs. It's done via a big network of layers, each of which composes a linear function with a nonlinearity. Each unit/neuron computes a weighted sum of its inputs and passes this sum through a nonlinear function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For inputs of dim $M$ and a layer with $N$ neurons, we need a weight matrix of shape $M \\times N$. Each layer in the network has an associated matrix of weights."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Learning is just optimization: the layer weights are adjusted in order to minimize a loss function; loss is computed between the true labels (in supervised learning) and the model predictions.\n",
    "\n",
    "SGD is a simple algorithm for this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The standard \"backprop\" algorithm is just the usual chain rule from calc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The visiual cortex was found to contain types of cells which detect hierarchies of features. This inspires convolutional network architecture, which is useful in imagery and text applications (where nearby data is related)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}